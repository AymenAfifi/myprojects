import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Charging data from .csv file with pandas library
# df stands for data frame
df = pd.read_csv("Churn_Modelling.csv")

# here we give same useful pandas commands to understand the data
print("______The first 5 rows are______")
print(df.head(5))

# If it is too long, use instead
print(df.head(5).T)

print("______The last 5 rows are______")
print(df.tail(5))

# Here just put the command inside a print
df.columns
df.index
df.shape

## Plotting some histograms and plots
#df.hist(figsize=(15,15));
#df.plot(figsize=(15,15));
#pd.plotting.scatter_matrix(df,figsize=(15,15));

# After the previous command we can choose the most relevant variables
# (features) such as the identifier of a client.
X = df.iloc[:,3:13].values

# y is the target variable (the dependant variable)
y = df.iloc[:,13].values

# X contains two categorical variables. We need to encode them.
# We start by a label encoder

from sklearn.preprocessing import LabelEncoder

label_encode_geography = LabelEncoder() 
label_encode_gender = LabelEncoder()


X[:,1] = label_encode_geography.fit_transform(X[:,1])
X[:,2] = label_encode_gender.fit_transform(X[:,2])

# The categorical variable gender has two values "Female" and "Male" so no pb
# The categorical variable geography has three values so pay attention about
# the dummy variable trap

# Here we use a One Hot Encoder to encode the geography variable
from sklearn.preprocessing import OneHotEncoder
onehotencoder = OneHotEncoder(categorical_features=[1])
X = onehotencoder.fit_transform(X).toarray()

"""
Avoiding the dummy variable trap
From one variable, the one hot encoding makes 3 correlated variables 
(3 columns), we need to suppress one of them. We choose the first one indexed
with 0
"""
X = X[:,1:]

# Now, we split our data into training data and testing data
# Here we use the train test split technic

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)


# Feature Scaling using the mean and the standard deviation
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
X_train = ss.fit_transform(X_train)
X_test = ss.transform(X_test)

# Here we implement the neural network so let's start with installing these
# three main libraries theano, tensorflow and keras

#!pip install theano
#!pip install tensorflow
#!pip install keras



# For this example, we will use the keras library with its backend tensorflow 
# Sequential  

import keras
from keras.models import Sequential
from keras.layers import Dense


# Initialization of Sequential a linear stack of layers.
classifier = Sequential()

# An artificial neural neutworks contains an input layer
# Adding the input layer and the first hidden layer
classifier.add(Dense(output_dim = 6,
                     init = 'uniform',
                     activation = 'relu',
                     input_dim = 11)
                )


# Adding of the second hidden layer
classifier.add(Dense(output_dim = 6,
                     init = 'uniform',
                     activation = 'relu')
                )

# Adding the second hidden layer
classifier.add(Dense(output_dim = 1,
                     init = 'uniform',
                     activation = 'sigmoid')
                )

# Compiling the ANN
classifier.compile(optimizer='adam',
                   loss='binary_crossentropy',
                   metrics = ['accuracy']
                   )

# Fitting the ANN to the training set
classifier.fit(X_train, y_train, batch_size=10, nb_epoch=100)

# Predicting the test set results
# here we obtain an array contains probabilities
y_pred = classifier.predict(X_test)

# Convert y_pred from probabilities to True or False
threshold = 0.5
y_pred = (y_pred > threshold)

# Now we can convert y_pred to 0 and 1 array by two methods
#1*y_pred simple multiplication by 1                      
#y_pred.astype(int) using a method

"""
for i in range(len(y_pred)):
    if y_pred[i] > 0.5:
        y_pred[i] = 1
    else:
        y_pred[i] = 0
"""

# Making the confusion matrix
from sklearn.metrics import confusion_matrix
cm1 = confusion_matrix(y_test, y_pred)

# ______ How to predict the behaviour of one customer ________ #
"""
Geography : France
Credit Score : 600
Gender : Male
Age : 40
Tenure : 3
Balance : 60000
Number of Products : 2
Has credit card : Yes
Is active member : Yes
Estimated Salary : 50000
"""

new_client = np.array([[0,0,600,1,40,3,60000,2,1,1,50000]])
new_client = ss.transform(new_client)
new_prediction = classifier.predict(new_client)
new_prediction = (new_prediction > 0.5)



import keras
# Evaluationg the ANN
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score
from keras.models import Sequential
from keras.layers import Dense

def build_classifier():
    classifier = Sequential()
    classifier.add(Dense(output_dim = 6,
                         init = 'uniform',
                         activation = 'relu',
                         input_dim = 11))
    classifier.add(Dense(output_dim = 6,
                         init = 'uniform',
                         activation = 'relu'))
    classifier.add(Dense(output_dim = 1,
                         init = 'uniform',
                         activation = 'sigmoid'))
    classifier.compile(optimizer = 'adam',
                       loss = 'binary_crossentropy',
                       metrics = ['accuracy'])
    return classifier

classifier = KerasClassifier(build_fn = build_classifier,
                             batch_size = 10,
                             nb_epoch = 100,
                             )

accuracies = cross_val_score(estimator = classifier,
                             X = X_train,
                             y = y_train,
                             cv = 20,
                             n_jobs = -1
                             )

# Improving the ANN
# We will use the dropout regularization to reduce overfitting if needed
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout

# Initialization of Sequential a linear stack of layers.
classifier = Sequential()

# An artificial neural neutworks contains an input layer
# Adding the input layer and the first hidden layer
classifier.add(Dense(output_dim = 6,
                     init = 'uniform',
                     activation = 'relu',
                     input_dim = 11)
                )

classifier.add(Dropout(p = 0.1))


# Adding of the second hidden layer
classifier.add(Dense(output_dim = 6,
                     init = 'uniform',
                     activation = 'relu')
                )

classifier.add(Dropout(p = 0.1))


# Adding the second hidden layer
classifier.add(Dense(output_dim = 1,
                     init = 'uniform',
                     activation = 'sigmoid')
                )

classifier.add(Dropout(p = 0.1))

# Compiling the ANN
classifier.compile(optimizer='adam',
                   loss='binary_crossentropy',
                   metrics = ['accuracy']
                   )

# Fitting the ANN to the training set
classifier.fit(X_train, y_train, batch_size=10, nb_epoch=100)

# Predicting the test set results
# here we obtain an array contains probabilities
y_pred = classifier.predict(X_test)

# Convert y_pred from probabilities to True or False
threshold = 0.5
y_pred = (y_pred > threshold)

# Now we can convert y_pred to 0 and 1 array by two methods
#1*y_pred simple multiplication by 1                      
#y_pred.astype(int) using a method

"""
for i in range(len(y_pred)):
    if y_pred[i] > 0.5:
        y_pred[i] = 1
    else:
        y_pred[i] = 0
"""

# Making the confusion matrix
from sklearn.metrics import confusion_matrix
cm1 = confusion_matrix(y_test, y_pred)

########
########
########

"""
import warnings
warnings.filterwarnings("ignore")
from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold
def Grid_Search_CV_RFR(X_train, y_train):
  #model
  model = XGBRegressor()
  #parameters
  param_grid = { 
          "n_estimators" : [10,20,30, 50],
          'max_depth': [4, 5, 6],
          'min_child_weight': [11],
          }

  grid = GridSearchCV(model, param_grid,
                      cv=KFold(n_splits=10, shuffle=True),
                      n_jobs = 1)

  grid.fit(X_train, y_train)
  return grid.best_estimator_, grid.best_score_ , grid.best_params_

Grid_Search_CV_RFR(X_train, y_train)
